{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIFAR-10: Image Dataset\n",
    "\n",
    "Throughout this course, we will teach you all basic skills and how to use all neccessary tools that you need to implement deep neural networks, which is the main focus of this class. However, you should also be proficient with handling data and know how to prepare it for your specific task. In fact, most of the jobs that involve deep learning in industry are very data related so this is an important skill that you have to pick up.\n",
    "\n",
    "Therefore, we will take a deep dive into data preparation this week by implementing our own datasets and dataloader. In this notebook, we will focus on the image dataset CIFAR-10. The CIFAR-10 dataset consists of 50000 32x32 colour images in 10 classes, which are *plane*, *car*, *bird*, *cat*, *deer*, *dog*, *frog*, *horse*, *ship*, *truck*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by importing some libraries that you will need along the way, as well as some code files that you will work on throughout this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from exercise_code.data import (\n",
    "    ImageFolderDataset,\n",
    "    RescaleTransform,\n",
    "    NormalizeTransform,\n",
    "    ComposeTransform,\n",
    "    compute_image_mean_and_std,\n",
    ")\n",
    "from exercise_code.tests import (\n",
    "    test_image_folder_dataset,\n",
    "    test_rescale_transform,\n",
    "    test_compute_image_mean_and_std,\n",
    "    test_len_dataset,\n",
    "    test_item_dataset,\n",
    "    test_transform_dataset,\n",
    "    save_pickle\n",
    ")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset Download\n",
    "Let us get started by downloading the data. In `exercise_code/data/image_folder_dataset.py` you can find a class `ImageFolderDataset`, which you will have to complete throughout this notebook.\n",
    "\n",
    "This class automatically downloads the raw data for you. To do so, simply initialize the class as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the output dataset folder\n",
    "i2dl_exercises_path = os.path.dirname(os.path.abspath(os.getcwd()))\n",
    "cifar_root = os.path.join(i2dl_exercises_path, \"datasets\", \"cifar10\")\n",
    "\n",
    "# Init the dataset and display downloading information this one time\n",
    "dataset = ImageFolderDataset(\n",
    "    root=cifar_root,\n",
    "    force_download=False,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should now be able to see the images in `i2dl_exercises/datasets/cifar10` in your file browser, which should contain one subfolder per class, each containing the respective images labeled `0001.png`, `0002.png`, ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, the dataset will only be downloaded the first time you initialize a dataset class. If, for some reason, your version of the dataset gets corrupted and you wish to re-download it, simply initialize the class with `force_download=True` in the download cell above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before training any model you should *always* take a look at some samples of your dataset. In this way, you can make sure that the data input has worked as intended and also get a feeling for the dataset. \n",
    "\n",
    "Let's load the CIFAR-10 data and visualize a subset of the images. To do so, `PIL.Image.open()` is used to open an image, and then `numpy.asarray()` to cast the image to a numpy array, which will have shape 32x32x3. In this way 7 images will be loaded per class, and then use `matplotlib.pyplot` to visualize those images in a grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_as_numpy(image_path):\n",
    "    return np.asarray(Image.open(image_path), dtype=float)\n",
    "\n",
    "classes = [\n",
    "    'plane', 'car', 'bird', 'cat', 'deer',\n",
    "    'dog', 'frog', 'horse', 'ship', 'truck',\n",
    "]\n",
    "num_classes = len(classes)\n",
    "samples_per_class = 7\n",
    "for label, cls in enumerate(sorted(classes)):\n",
    "    for i in range(samples_per_class):\n",
    "        image_path = os.path.join(\n",
    "            cifar_root,\n",
    "            cls,\n",
    "            str(i+1).zfill(4) + \".png\"\n",
    "        )  # e.g. cifar10/plane/0001.png\n",
    "        image = np.asarray(Image.open(image_path))  # open image as numpy array\n",
    "        plt_idx = i * num_classes + label + 1  # calculate plot location in the grid\n",
    "        plt.subplot(samples_per_class, num_classes, plt_idx)\n",
    "        plt.imshow(image.astype('uint8'))\n",
    "        plt.axis('off')\n",
    "        if i == 0:\n",
    "            plt.title(cls)  # plot class names above columns\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ImageFolderDataset Implementation\n",
    "\n",
    "Loading images following steps above is a bit cumbersome. Therefore, the next step is to write a custom **Dataset** class, which takes care of the data loading. This is always the first thing you have to implement when starting a new deep learning project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Dataset Class\n",
    "\n",
    "The **Dataset** class is a wrapper that loads the data from a given file path and returns a dictionary containing already prepared data, as you have done above. Datasets always need to have the following two methods implemented:\n",
    "- `__len__(self)` is a method that should simply calculate and return the number of images in the dataset. After it is implemented, you can simply call it with `len(dataset)`.\n",
    "- `__getitem__(self, index)` should return the image with the given index from your dataset. Implementing this will allow you to access your dataset like a list, i.e. you can then simply call `dataset[9]` to access the 10th image in the dataset.\n",
    "\n",
    "Generally, you will have to implement a different dataset for every project. However, base dataset classes for future projects will be provided for you in future projects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 ImageFolderDataset Implementation\n",
    "\n",
    "Now it is your turn to implement such a dataset class for CIFAR-10. To do so, open `exercise_code/data/image_folder_dataset.py` and check the following three methods of `ImageFolderDataset`:\n",
    "- `make_dataset(directory, class_to_idx)` should load the prepared data from a given directory root (`directory`) into two lists (`images` and `labels`). `class_to_idx` is a dict mapping class (e.g. 'cat') to label (e.g. 1).\n",
    "- `__len__(self)` should calculate and return the number of images in your dataset.\n",
    "- `__getitem__(self, index)` should return the image with the given index from your dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <h3>Task: Check Code</h3>\n",
    "    <p>Please read <code>make_dataset(directory, class_to_idx)</code> and make sure to familiarize with its output as you will need to interact with it for the following tasks. Additionally, it would be a wise decision to get familiar with python's os library which will be of utmost importance for most datasets you will write in future projects. As it is not beginner friendly, we removed it for this exercise but it is an important skill for a DL practicer.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <h3>Task: Implement</h3>\n",
    "    <p>Implement the <code>__len__(self)</code> method in <code>exercise_code/data/image_folder_dataset.py</code> and test your implementation by running the following cell.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exercise_code.data.image_folder_dataset import ImageFolderDataset\n",
    "\n",
    "dataset = ImageFolderDataset(\n",
    "    root=cifar_root,\n",
    ")\n",
    "\n",
    "_ = test_len_dataset(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <h3>Task: Implement</h3>\n",
    "    <p>Implement the <code>__getitem__(self, index)</code> method in <code>exercise_code/data/image_folder_dataset.py</code> and test your implementation by running the following cell.\n",
    "    </p>\n",
    "    <p><b>Hint:</b> You may want to reuse parts of the '2. Data Visualization' code above in your implementation of <code>__getitem__()</code>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exercise_code.data.image_folder_dataset import ImageFolderDataset\n",
    "\n",
    "dataset = ImageFolderDataset(\n",
    "    root=cifar_root,\n",
    ")\n",
    "\n",
    "_ = test_item_dataset(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Dataset Usage\n",
    "\n",
    "Now that you have implemented all required parts of the ImageFolderDataset, using the `__getitem__()` method, you can now access our dataset as conveniently as you would access a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_item = dataset[0]\n",
    "sample_image = sample_item[\"image\"]\n",
    "sample_label = sample_item[\"label\"]\n",
    "\n",
    "print('Sample image shape:', sample_image.shape)\n",
    "print('Sample label:', sample_label)\n",
    "print('Sample image first values:', sample_image[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the images are represented as uint8 values for each of the three RGB color channels. The data type and scale will be important later.\n",
    "\n",
    "As you have implemented both `__len__()` and `__getitem__()`, you can now even iterate over the dataset with a simple for loop! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 0\n",
    "for sample in tqdm(dataset):\n",
    "    num_samples += 1\n",
    "    \n",
    "print(\"Number of samples:\", num_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Transforms and Image Preprocessing\n",
    "\n",
    "Before training machine learning models, you often need to pre-process the data. For image datasets, two commonly applied techniques are:\n",
    "1. Normalize all images so that each value is either in [-1, 1] or [0, 1]. By doing so the image are also converted to floating point numbers.\n",
    "2. Compute the mean over all images and subtract this mean from all images in the dataset\n",
    "\n",
    "These transform classes are callables, meaning that you will be able to simply use them as follows:\n",
    "\n",
    "```transform = Transform()\n",
    "images_transformed = transform(images)```\n",
    "\n",
    "This will be realized in the pipeline by defining so called transforms. Instead of applying them globally to the input data, you will apply those seperatly to each sample after loading it in the `__getitem__` call of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <h3>Task: Implement</h3>\n",
    "    <p>Modify the <code>__getitem__(self, index)</code> method in <code>exercise_code/data/image_folder_dataset.py</code> such that it applies <code>self.transform</code>. With this change you can simply define the transforms during dataset creation and apply those automatically for each <code>__getitem__</code> call. Make sure not to break it though ;).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exercise_code.data.image_folder_dataset import ImageFolderDataset\n",
    "\n",
    "dataset = ImageFolderDataset(\n",
    "    root=cifar_root,\n",
    ")\n",
    "\n",
    "_ = test_transform_dataset(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Equipped with this change, you can now easily add the two preprocessing techniques above for CIFAR-10. You will do so in the following steps by implementing the classes `RescaleTransform` and `NormalizeTransform` in `exercise_code/data/transforms.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Rescaling Images using RescaleTransform\n",
    "\n",
    "Let's start by implementing `RescaleTransform`. If you look at the `__init__()` method, you will notice it has four arguments:\n",
    "* **out_range** is the range you wish to rescale your images to. E.g. if you want to scale your images to [-1, 1], you would use `range=(-1, 1)`. By default, they will be scaled to [0, 1].\n",
    "* **in_range** is the value range of the data prior to rescaling. For uint8 images, this will always be (0, 255)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <h3>Task: Implement</h3>\n",
    "    <p>Implement the <code>__call__()</code> method of <code>RescaleTransform</code> in <code>exercise_code/data/transforms.py</code> and test your implementation by running the following cell.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exercise_code.data.image_folder_dataset import ImageFolderDataset\n",
    "from exercise_code.data.transforms import RescaleTransform\n",
    "\n",
    "rescale_transform = RescaleTransform()\n",
    "dataset_rescaled = ImageFolderDataset(\n",
    "    root=cifar_root,\n",
    "    transform=rescale_transform\n",
    ")\n",
    "\n",
    "_ = test_rescale_transform(dataset_rescaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you look at the first image, you should now see that all values are between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_item = dataset_rescaled[0]\n",
    "sample_label = sample_item[\"label\"]\n",
    "sample_image = sample_item[\"image\"]\n",
    "\n",
    "print(\"Max value:\", np.max(sample_image))\n",
    "print(\"Min value:\", np.min(sample_image))\n",
    "print('Sample rescaled image first values:', sample_image[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Normalize Images to Standard Gaussian using NormalizeTransform\n",
    "\n",
    "Let us now move on to the `NormalizeTransform` class. The `NormalizeTransform` class normalizes images channel-wise and its `__init__` method has two arguments:\n",
    "* **mean** is the normalization mean, which will be subtracted from the dataset.\n",
    "* **std** is the normalization standard deviation. By scaling the data with a factor of `1/std` the standard deviation will be normazlied accordingly.\n",
    "\n",
    "Have a look at the code in `exercise_code/data/transforms.py`.\n",
    "\n",
    "The next step is to normalize the CIFAR-10 **images channel-wise** to standard normal. To do so, you need to calculate the **per-channel image mean and standard deviation** first, which you can then provide to `NormalizeTransform` to normalize the data accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You first have to load all rescaled images\n",
    "rescaled_images = []\n",
    "for sample in tqdm(dataset_rescaled):\n",
    "    rescaled_images.append(sample[\"image\"])\n",
    "rescaled_images = np.array(rescaled_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <h3>Task: Implement</h3>\n",
    "    <p>Implement the <code>compute_image_mean_and_std()</code> method and the <code>__call__()</code> method of <code>NormalizeTransform</code> in <code>exercise_code/data/transforms.py</code>. Compute the rescaled dataset's mean and variance by running the following cell.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exercise_code.data.transforms import compute_image_mean_and_std\n",
    "\n",
    "cifar_mean, cifar_std = compute_image_mean_and_std(rescaled_images)\n",
    "print(\"Mean:\\t\", cifar_mean, \"\\nStd:\\t\", cifar_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test your implementation, run the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = test_compute_image_mean_and_std(cifar_mean, cifar_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The rescaled images will be deleted now from your ram as they are no longer needed\n",
    "try:\n",
    "    del rescaled_images\n",
    "except NameError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can use the mean and standard deviation you computed to normalize the loaded data. This can be done by simply adding the `NormalizeTransform` to the list of transformations our dataset applies in `__getitem__()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <h3>Task: Check Code</h3>\n",
    "    <p>Please check out the <code>ComposeTransform</code> in <code>transforms.py</code>. Later on, we will most often use multiple transforms and chain them together. Remember that the order is of importance here!</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exercise_code.data.image_folder_dataset import ImageFolderDataset\n",
    "from exercise_code.data.transforms import RescaleTransform, NormalizeTransform, ComposeTransform\n",
    "\n",
    "# Set up both transforms using the parameters computed above\n",
    "rescale_transform = RescaleTransform()\n",
    "normalize_transform = NormalizeTransform(\n",
    "    mean=cifar_mean,\n",
    "    std=cifar_std\n",
    ")\n",
    "\n",
    "final_dataset = ImageFolderDataset(\n",
    "    root=cifar_root,\n",
    "    transform=ComposeTransform([rescale_transform, normalize_transform])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now check out the results of the transformed samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_item = final_dataset[0]\n",
    "sample_label = sample_item[\"label\"]\n",
    "sample_image = sample_item[\"image\"]\n",
    "\n",
    "print('Sample normalized image shape:', sample_image.shape)\n",
    "print('Sample normalized image first values:', sample_image[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Save your Dataset\n",
    "Now save your dataset and transforms using the following cell. This will save it to a pickle file `models/cifar_dataset.p`. We will use this dataset for the next notebook and this will count for the submission."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "    <h3>Note</h3>\n",
    "    <p>Each time you make changes in `dataset`, you need to rerun the following code to make your changes saved, but <b>this is NOT the file which you should submit</b>. You will find the final file for submission in the second notebook.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_pickle(\n",
    "    data_dict={\n",
    "        \"dataset\": final_dataset,\n",
    "        \"cifar_mean\": cifar_mean,\n",
    "        \"cifar_std\": cifar_std,\n",
    "    },\n",
    "    file_name=\"cifar_dataset.p\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Key Takeaways\n",
    "1. Always have a look at your data before you start training any models on it.\n",
    "2. Datasets should be organized in corresponding **Dataset** classes that support `__len__` and `__getitem__` methods, which allow us to call `len(dataset)` and `dataset[index]`.\n",
    "3. Data often needs to be preprocessed. Such preprocessing can be implemented in **Transform** classes, which are callables that can be simply applied via `data_transformed = transform(data)`. However, we will rarely do that and apply transforms on the fly using a dataloader which we will introduce in the next notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
